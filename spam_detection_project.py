# -*- coding: utf-8 -*-
"""Spam detection project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1utkXZ4XggpGG9QK2FBH38oycm8Gzzehw
"""

# Importing necessary libraries from the NLTK toolkit
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize  # For tokenizing text into words and sentences

# Importing stopwords from NLTK to remove common words that add little value
from nltk.corpus import stopwords

# Downloading required NLTK datasets
nltk.download('punkt')  # Downloads tokenizer models for sentence and word tokenization
nltk.download('punkt_tab')  # Optional: Additional support for tokenization
nltk.download('stopwords')  # Downloads predefined stopword lists for various languages

# Downloading the dataset from Kaggle using Kaggle CLI
# Here, we download a dataset containing spam emails
!kaggle datasets download -d abdallahwagih/spam-emails  # Downloads the spam emails dataset
!unzip spam-emails.zip  # Extracts the downloaded dataset

# Importing pandas for data manipulation
import pandas as pd

# Loading the dataset into a pandas DataFrame
# The CSV file contains columns like 'Message' (email text) and 'Label' (spam/not spam indicator)
df = pd.read_csv("spam.csv")

# Step to clean the text data
# - Removing punctuation, special characters, and multiple spaces
# - Preparing the data for tokenization and further processing
import re  # Regular expressions module for text cleaning

cleaned = []  # List to store cleaned text
for text in df['Message']:  # Iterating over each message in the 'Message' column
    cleaned_text = re.sub(r'[^\w\s]', '', text)  # Removing punctuation and special characters
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text)  # Replacing multiple spaces with a single space
    cleaned_data = cleaned_text.strip()  # Removing leading and trailing spaces
    cleaned.append(cleaned_data)  # Appending the cleaned text to the list

# Tokenizing the cleaned text into words
# Each cleaned text is split into a list of individual words for further analysis
tokens = [word_tokenize(x) for x in cleaned]

# Removing stopwords from the tokenized words
# Stopwords are common words (e.g., "the", "is") that do not contribute much to analysis
stop = set(stopwords.words('english'))  # Fetching the list of English stopwords
stop_token = []  # List to store tokens after removing stopwords
for k in range(len(df['Message'])):  # Iterating through tokenized text
    p = [i for i in tokens[k] if i not in stop]  # Filtering out stopwords
    stop_token.append(p)  # Adding filtered tokens to the list

# Applying stemming to reduce words to their root form
# This helps group similar words (e.g., "running", "runner" -> "run")
from nltk.stem import PorterStemmer
ps = PorterStemmer()  # Initializing the Porter Stemmer
stemedata = []  # List to store stemmed data
for message in stop_token:  # Iterating over tokens after stopword removal
    st = [ps.stem(word) for word in message]  # Applying stemming to each word
    stemedata.append(st)  # Adding the stemmed tokens to the list

# Summary of steps:
# 1. Necessary libraries and NLTK datasets are imported/downloaded.
# 2. A spam email dataset is downloaded from Kaggle and loaded into a pandas DataFrame.
# 3. The 'Message' column is cleaned by removing punctuation, special characters, and extra spaces.
# 4. The cleaned text is tokenized into individual words.
# 5. Stopwords are removed from the tokenized words to reduce noise.
# 6. Stemming is applied to group similar words to their root form.

from nltk import pos_tag

nltk.download('averaged_perceptron_tagger_eng')

stop_token[0]

pos_tokens=pos_tag(stop_token[0])
pos_tokens

nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lm=WordNetLemmatizer()

lm.lemmatize('running',pos='v')

lm.lemmatize('fastest',pos='a')

from nltk.corpus import wordnet
# Function to convert nltk's POS tags to WordNet's POS tags
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ  # Adjective
    elif tag.startswith('V'):
        return wordnet.VERB  # Verb
    elif tag.startswith('N'):
        return wordnet.NOUN  # Noun
    elif tag.startswith('R'):
        return wordnet.ADV  # Adverb
    else:
        return wordnet.NOUN  # Default to noun if
get_wordnet_pos(pos_tokens[5][1])

lemmed_data = []
for x in range(len(pos_tokens)):  # Iterate through the outer list (pos_tokens)
    word = pos_tokens[x][0]  # Access the word
    tag = pos_tokens[x][1]  # Access the POS tag
    lemmatized_word = lm.lemmatize(word, get_wordnet_pos(tag))  # Lemmatize the word
    lemmed_data.append(lemmatized_word)  # Append the lemmatized word to the result list

# Output the lemmatized data
print("POS Tokens:", pos_tokens)
print("Lemmatized Data:", lemmed_data)

import numpy as np
class Perceptron:
    def _init_(self,X,y,a,iteration):
        self.X = X
        self.y = y
        self.a = a
        self.iteration = iteration
    def fit(self):
        X_t = self.X.T
        on = np.zeros(self.X.shape[0]) #theta n(1,2,3,4)
        o0 = 0
        for i in range(self.iteration):
            on = on-(self.a*(np.sum(((np.dot(X_t,on)+o0)-self.y)*self.X,axis=1)/self.X.shape[1]))
            o0 = o0-(self.a*(np.sum(((np.dot(X_t,on)+o0)-self.y)*1)/self.X.shape[1]))
            print(on,o0)
        self.on = on
        self.o0 = o0
        return {"theta":on,"theta0":o0}
    def predict(self,var):# [500000,50000]*[]
        res = np.sum((self.on*var),axis=0)+self.o0
        return res

# prompt: linear regression

import numpy as np

class Perceptron:
    def __init__(self, X, y, a, iteration):
        self.X = X
        self.y = y
        self.a = a
        self.iteration = iteration
        self.on = None  # Initialize on and o0
        self.o0 = None

    def fit(self):
        X_t = self.X.T
        on = np.zeros(self.X.shape[1])  # Initialize on with the correct shape
        o0 = 0
        for i in range(self.iteration):
            on = on - (self.a * (np.sum(((np.dot(X_t, on) + o0) - self.y) * X_t, axis=1) / self.X.shape[0]))
            o0 = o0 - (self.a * (np.sum(((np.dot(X_t, on) + o0) - self.y) * 1) / self.X.shape[0]))
            print(on, o0)
        self.on = on
        self.o0 = o0
        return {"theta": on, "theta0": o0}

    def predict(self, var):  # [500000,50000]*[]
        res = np.dot(self.on, var) + self.o0  # Use np.dot for matrix multiplication
        return res

