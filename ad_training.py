# -*- coding: utf-8 -*-
"""AD Training

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iL_gXgmXjQQ8ReqqHibxi9jkH8zxPjCh
"""

import nltk
nltk.download('punkt_tab')
!kaggle datasets download -d abdallahwagih/spam-emails
!unzip spam-emails.zip

import pandas as pd
df=pd.read_csv('spam.csv')
df

x=df.iloc[2,1]
x

from nltk.tokenize import word_tokenize, sent_tokenize
sent_tokenize(x)

word_tokenize(x)

for i in df['Message']:
  print(i)
  sent_tokenize(i)

word_tokenize(i)

sentTokens=[sent_tokenize(i) for i in df['Message']]
sentTokens

wordTokens=[word_tokenize(i) for i in df['Message']]
wordTokens

df1=pd.read_csv('https://github.com/singhrau0/Big-Data-Preprocessing/raw/refs/heads/main/youtubesample.csv')
df1

y=df1.iloc[2,15]
y

sentokens=[sent_tokenize(y) for j in df1['description']]
sentokens

wordtokens=[word_tokenize(y) for j in df1['description']]
wordtokens

text=df['Message'][5]
text

#/21/01/25/
# Remove special characters and punctuation using regex
import re
def clean_text(text):
    cleaned_text = re.sub(r'[^\w\s]', '', text)  # Removes everything except word characters and spaces
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text)  # Replaces multiple spaces with a single space
    return cleaned_text.strip()  # Removes leading/trailing whitespace
result=clean_text(text)
result

mydata=[]
for i in df['Message']:
  cleaned_text = re.sub(r'[^\w\s]', '', i)  # Removes everything except word characters and spaces
  cleaned_text = re.sub(r'\s+', ' ', cleaned_text)  # Replaces multiple spaces with a single space
  cleaned_data= cleaned_text.strip()
  mydata.append(cleaned_data)

mydata

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize, sent_tokenize

m=word_tokenize(mydata[0])
m

nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
stop_words



from nltk import word_tokenize
nltk.download('punkt')
nltk.download('punkt_tab')

tokens=[word_tokenize(i) for i in mydata]
tokens

[i for i in tokens[0] if i not in stop_words]

#x is message
data=[]
for x in tokens:
  cl=[i for i in x if i not in stop_words]
  data.append(cl)
data

#Stemming process on data
from nltk.stem import PorterStemmer
ps=PorterStemmer()
data[0]

ps.stem(data[0][4])

for i in data[0]:
  print(ps.stem(i))

for j in data:
  for i in j:
    print(ps.stem(i))

stemeddata=[[ps.stem(i) for i in j] for j in data]
stemeddata

#lemetization
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

lem=WordNetLemmatizer()
lem.lemmatize('better',pos='a')

lem.lemmatize('running',pos='v')

lem.lemmatize('studied',pos='v')

lem.lemmatize('fastly',pos='a')

nltk.download('averaged_perceptron_tagger')
nltk.download('averaged_perceptron_tagger_eng')

sen="The quick brown fox jumps over the lazy dog"
words=word_tokenize(sen)
words
tags=nltk.pos_tag(words)
print(tags)

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk import pos_tag

# Assuming mydata is a list of strings
sentences = sent_tokenize(mydata[0])  # Tokenize into sentences
tokens = [word_tokenize(sentence) for sentence in sentences]  # Tokenize each sentence into words
pos_tags = [pos_tag(token_list) for token_list in tokens]  # Perform POS tagging on each tokenized sentence

# Example output
for sentence_tags in pos_tags:
    print(sentence_tags)

